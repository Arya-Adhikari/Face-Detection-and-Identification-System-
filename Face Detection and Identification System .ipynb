{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8e35f2d-9303-4ee1-a2b4-c6a4825d72c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# pip install opencv-python tensorflow keras scikit-learn matplotlib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2761cfe4-1228-49c8-9da0-035760dbc082",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import essential libraries in the notebook\n",
    "\n",
    "import cv2\n",
    "import numpy as np\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4381e2bb-ef53-4389-b354-0ab0bce3a53c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize the webcam using OpenCV\n",
    "\n",
    "cap = cv2.VideoCapture(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79ae8ffa-5259-4487-85bd-355b2a18d989",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Capture frames\n",
    "\n",
    "ret, frame = cap.read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "efef1a13-169a-4b5b-b3b3-65f4f7374efb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# code for capturing and saving images\n",
    "\n",
    "import cv2\n",
    "import os\n",
    "\n",
    "# Create directories for each person (label)\n",
    "label = \"Arya\"  # Change this for different classes\n",
    "output_dir = f\"./data/{label}\"\n",
    "os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "cap = cv2.VideoCapture(0)\n",
    "print(\"Press 's' to save the image, 'q' to quit.\")\n",
    "\n",
    "image_count = 0\n",
    "while True:\n",
    "    ret, frame = cap.read()\n",
    "    if not ret:\n",
    "        break\n",
    "    cv2.imshow(\"Webcam\", frame)\n",
    "    key = cv2.waitKey(1) & 0xFF\n",
    "    \n",
    "    if key == ord('s'):  # Save the frame\n",
    "        image_path = os.path.join(output_dir, f\"{label}_{image_count}.jpg\")\n",
    "        cv2.imwrite(image_path, frame)\n",
    "        image_count += 1\n",
    "        print(f\"Saved: {image_path}\")\n",
    "    \n",
    "    elif key == ord('q'):  # Quit the capture\n",
    "        break\n",
    "\n",
    "cap.release()\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80ac13c5-ccf6-490a-ba36-b97a69fe1774",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Path to the directory where images are saved\n",
    "image_dir = \"./data/Arya\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "749b745c-9727-4577-857f-a4e6c4c297f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the latest saved image from the directory\n",
    "images = sorted(os.listdir(image_dir), key=lambda x: os.path.getctime(os.path.join(image_dir, x)))\n",
    "if len(images) == 0:\n",
    "    print(\"No images found in the directory.\")\n",
    "    exit()\n",
    "\n",
    "latest_image_path = os.path.join(image_dir, images[-1])\n",
    "print(f\"Using image: {latest_image_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5524a68-fa76-4df8-a2c8-c631f50a22b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read the latest image\n",
    "image = cv2.imread(latest_image_path)\n",
    "gray = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19c17fbc-48af-4138-a115-6248d032a030",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the Haar Cascade face detection model using OpenCV.\n",
    "# This model will be used to detect faces in the captured images.\n",
    "\n",
    "face_cascade = cv2.CascadeClassifier(cv2.data.haarcascades + 'haarcascade_frontalface_default.xml')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "532efc52-c973-45ae-834e-9d9cb86c44a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Perform face detection\n",
    "faces = face_cascade.detectMultiScale(gray, scaleFactor=1.1, minNeighbors=5, minSize=(30, 30))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0cf70f3a-6a8b-4630-9c0a-5a9377ea28cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Perform face detection with adjusted parameters\n",
    "faces = face_cascade.detectMultiScale(\n",
    "    gray,\n",
    "    scaleFactor=1.05,  # Reduce the scale step for finer detection\n",
    "    minNeighbors=6,    # Increase for stricter face detection\n",
    "    minSize=(50, 50)   # Minimum face size to detect\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "846d5830-004c-4722-8a50-7560570507ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Draw bounding boxes around detected faces\n",
    "for (x, y, w, h) in faces:\n",
    "    cv2.rectangle(image, (x, y), (x + w, y + h), (255, 0, 0), 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64841918-a38d-40d6-932d-5b1b4239955c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display the image with bounding boxes\n",
    "cv2.imshow(\"Detected Faces\", image)\n",
    "cv2.waitKey(0)  # Wait for a key press to close the window\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "474236d6-a20b-4d89-86bc-cd3253dccb18",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Draw bounding boxes and add labels\n",
    "if len(faces) == 0:\n",
    "    print(\"No faces detected.\")\n",
    "else:\n",
    "    for (x, y, w, h) in faces:\n",
    "        # Draw rectangle with custom color and thickness\n",
    "        color = (0, 255, 0)  # Green color for the box\n",
    "        thickness = 2\n",
    "        cv2.rectangle(image, (x, y), (x + w, y + h), color, thickness)\n",
    "        \n",
    "        # Add label above the rectangle\n",
    "        label = \"Face\"\n",
    "        font = cv2.FONT_HERSHEY_SIMPLEX\n",
    "        font_scale = 0.5\n",
    "        font_thickness = 1\n",
    "        label_size, _ = cv2.getTextSize(label, font, font_scale, font_thickness)\n",
    "        \n",
    "        label_x = x\n",
    "        label_y = y - 10 if y - 10 > 10 else y + 10  # Adjust position if label goes out of bounds\n",
    "        cv2.rectangle(image, (label_x, label_y - label_size[1] - 2), (label_x + label_size[0], label_y + 2), color, cv2.FILLED)\n",
    "        cv2.putText(image, label, (label_x, label_y), font, font_scale, (0, 0, 0), font_thickness)\n",
    "\n",
    "# Display the image with bounding boxes and labels\n",
    "cv2.imshow(\"Detected Faces\", image)\n",
    "cv2.waitKey(0)  # Wait for a key press to close the window\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "251fecfb-afa5-4824-963e-6c466bc11143",
   "metadata": {},
   "outputs": [],
   "source": [
    "output_dir = './processed_faces/'  # Directory to save preprocessed faces\n",
    "os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "if len(faces) == 0:\n",
    "    print(\"No faces detected.\")\n",
    "else:\n",
    "    # Loop through detected faces\n",
    "    for idx, (x, y, w, h) in enumerate(faces):\n",
    "        # Crop the face\n",
    "        cropped_face = image[y:y+h, x:x+w]\n",
    "        \n",
    "        # Resize to a uniform size (e.g., 160x160 for models like FaceNet)\n",
    "        resized_face = cv2.resize(cropped_face, (160, 160))\n",
    "        \n",
    "        # Save the preprocessed face\n",
    "        face_path = os.path.join(output_dir, f\"face_{idx}.jpg\")\n",
    "        cv2.imwrite(face_path, resized_face)\n",
    "        print(f\"Saved preprocessed face: {face_path}\")\n",
    "        \n",
    "        # Display the preprocessed face (optional)\n",
    "        cv2.imshow(\"Preprocessed Face\", resized_face)\n",
    "        cv2.waitKey(0) # The 0 argument tells OpenCV to wait indefinitely for the user to press any key\n",
    "\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab007ab0-9ff1-4f1b-8783-480a9d7da3c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load a pre-trained model\n",
    "\n",
    "from tensorflow.keras.applications import MobileNetV2\n",
    "\n",
    "model = MobileNetV2(weights='imagenet', \n",
    "                    include_top=False, \n",
    "                    input_shape=(160, 160, 3))\n",
    "\n",
    "# If you have downloaded a model\n",
    "# model = load_model('model.h5')  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7970f8b1-1cf9-4b23-bbd1-20490c1ae4e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize lists for embeddings and labels\n",
    "embeddings = []\n",
    "labels = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a6291f6-822e-40cb-8629-3fc80a3b7c4f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# define the folder for preprocessed faces\n",
    "processed_faces_dir = './processed_faces'\n",
    "\n",
    "# Generate embeddings for all preprocessed faces\n",
    "\n",
    "# Iterate through all images in the directory\n",
    "for image_name in os.listdir(processed_faces_dir):\n",
    "    image_path = os.path.join(processed_faces_dir, image_name)\n",
    "\n",
    "    # Check if the file is a valid image (optional: you can adjust based on file types)\n",
    "    if image_name.endswith('.jpg') or image_name.endswith('.jpeg') or image_name.endswith('.png'):\n",
    "        # Load and preprocess image\n",
    "        face_image = cv2.imread(image_path)\n",
    "        face_image = cv2.resize(face_image, (160, 160))  # Ensure correct input size\n",
    "        face_preprocessed = face_image.astype('float32') / 255.0  # Normalize\n",
    "        \n",
    "        # Generate embedding\n",
    "        embedding = model.predict(np.expand_dims(face_preprocessed, axis=0))[0]\n",
    "        embeddings.append(embedding)\n",
    "        \n",
    "        # Use the image filename as the label (without file extension)\n",
    "        label = os.path.splitext(image_name)[0]\n",
    "        labels.append(label)\n",
    "\n",
    "print(f\"Generated embeddings for {len(embeddings)} faces.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dbb5f079-3d14-4f69-ae0d-dd10f1363cd0",
   "metadata": {},
   "outputs": [],
   "source": [
    "labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a737c92-0004-46e2-ba80-f32aa1750ab0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "\n",
    "# Train a KNN classifier\n",
    "knn = KNeighborsClassifier(n_neighbors=3)\n",
    "knn.fit(embeddings, labels)\n",
    "print(\"Classifier trained.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0a4ada6-abc2-44be-af97-19c0ebd4c3fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test on a new face\n",
    "new_face = cv2.imread('./processed_faces/test_face.jpg')\n",
    "new_embedding = get_embedding(facenet_model, new_face)\n",
    "\n",
    "# Predict the identity\n",
    "predicted_label = knn.predict([new_embedding])\n",
    "print(\"Predicted Identity:\", predicted_label[0])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
